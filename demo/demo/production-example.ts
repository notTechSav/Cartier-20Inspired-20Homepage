/**
 * Production-ready example using the centralized AI configuration
 * This is the RECOMMENDED pattern for real applications
 */

import { generateText, streamText } from 'ai';
import { models, getModel } from './lib-ai';

async function main() {
  console.log('üöÄ Production Pattern Demo\n');

  // Example 1: Using getModel() directly
  console.log('1Ô∏è‚É£ Using getModel() function...');
  const result1 = await generateText({
    model: getModel('gpt-4o-mini'),
    prompt: 'Say hello in 3 words',
  });
  console.log('Response:', result1.text);
  console.log('');

  // Example 2: Using the model shortcuts (recommended!)
  console.log('2Ô∏è‚É£ Using models.gpt4oMini() shortcut...');
  const result2 = await generateText({
    model: models.gpt4oMini(),
    prompt: 'Count from 1 to 5',
  });
  console.log('Response:', result2.text);
  console.log('');

  // Example 3: Streaming with the production pattern
  console.log('3Ô∏è‚É£ Streaming response...');
  const { textStream } = await streamText({
    model: models.gpt4oMini(),
    prompt: 'Write a haiku about code',
  });

  for await (const chunk of textStream) {
    process.stdout.write(chunk);
  }
  console.log('\n');

  // Example 4: Chat conversation
  console.log('4Ô∏è‚É£ Multi-turn conversation...');
  const result4 = await generateText({
    model: models.gpt4oMini(),
    messages: [
      { role: 'system', content: 'You are a helpful coding assistant.' },
      { role: 'user', content: 'What is TypeScript?' },
    ],
  });
  console.log('Response:', result4.text);
  console.log('\nüìä Usage:', result4.usage);

  console.log('\n‚úÖ All examples completed!');
  console.log('üí° All requests were routed through Vercel AI Gateway');
  console.log('üìà Check your Vercel dashboard for analytics!');
}

main().catch(console.error);

